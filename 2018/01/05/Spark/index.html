<!doctype html>



  


<html class="theme-next pisces use-motion" lang="zh-Hans">
<head>
  <meta charset="UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=edge" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>



<meta http-equiv="Cache-Control" content="no-transform" />
<meta http-equiv="Cache-Control" content="no-siteapp" />















  
  
  <link href="/lib/fancybox/source/jquery.fancybox.css?v=2.1.5" rel="stylesheet" type="text/css" />




  
  
  
  

  
    
    
  

  

  

  

  

  
    
    
    <link href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&subset=latin,latin-ext" rel="stylesheet" type="text/css">
  






<link href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2" rel="stylesheet" type="text/css" />

<link href="/css/main.css?v=5.1.0" rel="stylesheet" type="text/css" />


  <meta name="keywords" content="Hadoop," />





  <link rel="alternate" href="/atom.xml" title="思 见" type="application/atom+xml" />




  <link rel="shortcut icon" type="image/x-icon" href="/favicon.ico?v=5.1.0" />






<meta name="description" content="Hadoop系列五：Spark基础">
<meta name="keywords" content="Hadoop">
<meta property="og:type" content="article">
<meta property="og:title" content="Spark">
<meta property="og:url" content="http://yoursite.com/2018/01/05/Spark/index.html">
<meta property="og:site_name" content="思 见">
<meta property="og:description" content="Hadoop系列五：Spark基础">
<meta property="og:locale" content="zh-Hans">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/22.BerkeleyDataAnalyticsStack.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/23.Spark老师讲述的MR.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/30.术语归纳.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/25.spark运行时.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/24.持久化策略-1.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/24.持久化策略-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/26.spark的yarn调度.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/31.spark源码解析概览.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/32.DAG的Stage切割核心代码.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/33.源码-1.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/33.源码-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/33.源码-3.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/33.源码-4.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/27.依据宽依赖的切割.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/28.转换算子与操作算子.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.2个shffule分成3个stage.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/34.pi.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.broadcast.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.accumulator.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/36.SparkSQL.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/38.SparkSQL底层.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/39.SparkSQL的优化.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/40.scala的隐式转换implicit.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/37.rdd与dataframe.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/41.jdbc运行.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.Hive数据源.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.开窗函数.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.UDAF-1.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.UDAF-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/35.UDAF-3.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/42.pageRank.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/42.pageRank-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/43.spark%20Stream架构.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/44.StreamWorldCount.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/44.StreamWorldCount-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/45.updateStateByKey.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/45.Tranforme.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/45.Tranforme-2.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/46.存储.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/46.数据库连接池.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/47.kafkaWordCount.png">
<meta property="og:image" content="http://yoursite.com/2018/01/05/Spark/47.directiStream读取方式.png">
<meta property="og:updated_time" content="2019-01-29T09:53:20.432Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Spark">
<meta name="twitter:description" content="Hadoop系列五：Spark基础">
<meta name="twitter:image" content="http://yoursite.com/2018/01/05/Spark/22.BerkeleyDataAnalyticsStack.png">



<script type="text/javascript" id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    sidebar: {"position":"left","display":"post","offset":12,"offset_float":0,"b2t":false,"scrollpercent":false},
    fancybox: true,
    motion: true,
    duoshuo: {
      userId: '0',
      author: '博主'
    },
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>



  <link rel="canonical" href="http://yoursite.com/2018/01/05/Spark/"/>





  <title> Spark | 思 见 </title>
</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-Hans">

  














  
  
    
  

  <div class="container one-collumn sidebar-position-left page-post-detail ">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta ">
    

    <div class="custom-logo-site-title">
      <a href="/"  class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">思 见</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
      
        <p class="site-subtitle"></p>
      
  </div>

  <div class="site-nav-toggle">
    <button>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>

<nav class="site-nav">
  

  
    <ul id="menu" class="menu">
      
        
        <li class="menu-item menu-item-home">
          <a href="/" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-home"></i> <br />
            
            首页
          </a>
        </li>
      
        
        <li class="menu-item menu-item-categories">
          <a href="/categories" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-th"></i> <br />
            
            分类
          </a>
        </li>
      
        
        <li class="menu-item menu-item-tags">
          <a href="/tags" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-tags"></i> <br />
            
            标签
          </a>
        </li>
      
        
        <li class="menu-item menu-item-about">
          <a href="/about" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-user"></i> <br />
            
            关于
          </a>
        </li>
      
        
        <li class="menu-item menu-item-archives">
          <a href="/archives" rel="section">
            
              <i class="menu-item-icon fa fa-fw fa-archive"></i> <br />
            
            归档
          </a>
        </li>
      

      
    </ul>
  

  
</nav>



 </div>
    </header>

    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  <article class="post post-type-normal " itemscope itemtype="http://schema.org/Article">
    <link itemprop="mainEntityOfPage" href="http://yoursite.com/2018/01/05/Spark/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="sun">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/curiosity.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="思 见">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
            
            
              
                Spark
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">
            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">发表于</span>
              
              <time title="创建于" itemprop="dateCreated datePublished" datetime="2018-01-05T17:16:24+08:00">
                2018-01-05
              </time>
            

            

            
          </span>

          
            <span class="post-category" >
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">分类于</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/知识/" itemprop="url" rel="index">
                    <span itemprop="name">知识</span>
                  </a>
                </span>

                
                
              
            </span>
          

          
            
          

          
          

          

          

          

        </div>
      </header>
    


    <div class="post-body" itemprop="articleBody">

      
      

      
        <p>Hadoop系列五：Spark基础</p>
<a id="more"></a>
<h1 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h1><ul>
<li>伯克利大学实验室研发，为机器学习的快速计算而设计</li>
<li>DataAnalytics Stack<br>Mesos: 相当于Yarn，Mesos是粗细粒度，Yarn是粗粒度的。Mesos比Yarn效率更高一些<pre><code>Mesos是C++开发，Yarn是Java开发
</code></pre>HDFS:分布式文件系统<br>Tachyon:内存文件系统，百度用的比较多<br>Spark Core/MR Storm<br>SPark SQL(Hive),MLLib(Mahout) ,Spark Streaming<br><img src="22.BerkeleyDataAnalyticsStack.png" alt="BerkeleyDataAnalyticsStack"></li>
</ul>
<h2 id="支持3种API"><a href="#支持3种API" class="headerlink" title="支持3种API"></a>支持3种API</h2><ul>
<li>scala很好</li>
<li>python不错</li>
<li>java也不错</li>
</ul>
<h2 id="4种模式"><a href="#4种模式" class="headerlink" title="4种模式"></a>4种模式</h2><ul>
<li>Local 多用于测试</li>
<li>standalone:独立的搭建，独立与hadoop</li>
<li>mesos</li>
<li>yarn：中国用的最多</li>
</ul>
<h1 id="回顾MR"><a href="#回顾MR" class="headerlink" title="回顾MR"></a>回顾MR</h1><p>  每个Map对应一个split，每个split大小（split_max/min Block Size）大小与BlockSize相同。大小在3×128MB～4×128MB<br>  右边3个ReduceTack<br>  buffer in memory是100MB，80%溢写<br>  map在写到缓冲区之前进行patition：哈希模运算，hash(key)%3<br>  溢写过程中，sort排序然后溢写到磁盘，每一溢写<br>  merge将这些小文件合并<br>  reduce端，来fetch拉去数据。reduce端将从各个map拉取来的数据进行sort排序合并成一个。merge完之后进行Group分组，分了多少组就会被调用多少次。</p>
<p>  MR慢的原因：</p>
<pre><code>1. 太多的磁盘IO:spill to dist,merge合并,fetch,sort,output
2. 大量的排序：强制的加入的排序--&gt;为了分组。使进入rudece函数的key相同。    
   map端的排序缓解reduce端压力
   排序是归并排序
3. shuffle阶段spark也避免不了，主要是每次Job的输入输出。
4. hdfs如何知道机器的框架？不知道,配置的。
</code></pre><p>  Spark快的原因<br>    1.内存计算<br>      hdfs读取，每次迭代放内存里，不够时也会放在磁盘里（配置）。<br>    2.DAG优化<br><img src="23.Spark老师讲述的MR.png" alt="Spark老师讲述的MR"></p>
<h1 id="sparkCore"><a href="#sparkCore" class="headerlink" title="sparkCore"></a>sparkCore</h1><h2 id="RDD"><a href="#RDD" class="headerlink" title="RDD"></a>RDD</h2><h3 id="（Resilient-Distributed-Dataset）弹性分布式数据集"><a href="#（Resilient-Distributed-Dataset）弹性分布式数据集" class="headerlink" title="（Resilient Distributed Dataset）弹性分布式数据集"></a>（Resilient Distributed Dataset）弹性分布式数据集</h3><h3 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h3><ul>
<li>a list of partitions(n篇数据,固定在某节点里:某个节点里的某块连续数据):一般是一个hdfs的block对应一个partitions，一般遵循数据的本地性。</li>
<li>a function for computiong each split（partitons=split从数据角度，mr的切割，算子：map、filter,persist）</li>
<li>a list of dependencies on other RDDS(不同算子将RDD变成不同的RDD，用于重新计算，内存中RDD不稳定缘故，出现宕机会重算)</li>
<li>Optionly, a Partitioner for key-value RDDS（可对RDD重新分区，增加并发）</li>
<li><p>Optionly, a list of preferred locations to compute each on（一个期望的位置去计算，数据本地性，如hdfs3个备份，3个节点都是期望的计算位置）</p>
</li>
<li><p>第四、五个每种rdd可以有也可以没有</p>
</li>
</ul>
<h3 id="RDD容错"><a href="#RDD容错" class="headerlink" title="RDD容错"></a>RDD容错</h3><ul>
<li>重新算</li>
<li>persist</li>
<li>checkPoint</li>
</ul>
<h2 id="术语解释"><a href="#术语解释" class="headerlink" title="术语解释"></a>术语解释</h2><p><img src="30.术语归纳.png" alt="术语归纳"></p>
<ul>
<li>application<br>sparckContext,包含driver，和集群上的executor</li>
<li>Driver:<br>DAGScheduler<br>TaskScheduler</li>
<li>clusterManager<br>在集群上获取资源的外部服务，例如standalone，memos，yarn</li>
<li>worker Node<br>运行代码的节点</li>
<li><p>executor（containnor）<br>在worker上为某应用程序启动的一个进程<br>每个应用程序都有各自独立的executors</p>
</li>
<li><p>job<br>包含多个task的并行计算，每个action</p>
</li>
<li>stage<br>一个job根据宽依赖切割成的stage</li>
<li>task<br>每个executor执行的最小单元</li>
</ul>
<h2 id="调度概览"><a href="#调度概览" class="headerlink" title="调度概览"></a>调度概览</h2><h3 id="运行时"><a href="#运行时" class="headerlink" title="运行时"></a>运行时</h3><p>每个application都有一个Dirver进行调度，它将job分成task，交给各个节点去运行，并收集返回result，输出结果。<br><img src="25.spark运行时.png" alt="spark运行时"></p>
<h3 id="流程示意图"><a href="#流程示意图" class="headerlink" title="流程示意图"></a>流程示意图</h3><ul>
<li>加载数据集</li>
<li><p>RDD转换，算子操作</p>
<ol>
<li>transformations延迟执行<br>2.action触发执行，立即执行</li>
</ol>
</li>
<li><p>实例<br>lines.sc.textFile(“filepath”):加载文件，成为RDD<br>errors = lines.filter(_.startWith(“ERROR”))：filter是transformation转换<br>errors.persis():缓存RDD<br>Mysql_errors = errors.filter(_.contain(“MySQL”)).count()：action执行<br>— job0</p>
<p>Http_errors = errors.filter(_.contain(“Http”)).count();<br>— job1</p>
<p>spark每次碰到acion把之前的运输封装成job，去执行。job0与job1是串行执行的？FIFO策略，Fair是并发执行<br>rdd其实是一个内存里一个瞬时的状态，以job0为例<br>若不持久化，job1会重新从hdfs中读数据，所以需要persist一下。<br>rdd复用时，需要持久化persist</p>
</li>
</ul>
<h2 id="持久化策略"><a href="#持久化策略" class="headerlink" title="持久化策略"></a>持久化策略</h2><p>StorageLevel<br> 1.磁盘 2.内存 3.使用JVM的堆，4.不序列化，4.副本数<br>memory_only：默认<br>memory_only_ser:序列化占内存少一点<br>MEMORY_AND_DISK，disk指的是本地磁盘，而不是hdfs，意思为先存内存，内存不够存磁盘。若运算时间超过读取时间，还是读取合适，反之应该计算。<br>读取时间 trade off 运算时间<br>一般当rdd被复用时，持久化对象<br>当数据做一定的容错使用_2</p>
<p><img src="24.持久化策略-1.png" alt="持久化策略1"><br><img src="24.持久化策略-2.png" alt="持久化策略2"></p>
<h3 id="序列化"><a href="#序列化" class="headerlink" title="序列化"></a>序列化</h3><ul>
<li>情况：<br>主从<br>_SER<br>shuffle网络传输</li>
<li>配置<br>conf.set(“saprk.serialzer”,”org.apache.spark.serialzer.Kryoserializer”)</li>
</ul>
<h2 id="搭建"><a href="#搭建" class="headerlink" title="搭建"></a>搭建</h2><h3 id="单机模式"><a href="#单机模式" class="headerlink" title="单机模式"></a>单机模式</h3><p><code>./bin/spark-submit --class org.apache.spark.examples.SparkPi --master local[*] ./lib/spark-examples-1.6.3-hadoop2.4.0.jar 100</code></p>
<h3 id="standalone集群模式"><a href="#standalone集群模式" class="headerlink" title="standalone集群模式"></a>standalone集群模式</h3><h4 id="配置文件"><a href="#配置文件" class="headerlink" title="配置文件"></a>配置文件</h4><ol>
<li>slaves<br>node2<br>node3</li>
<li>spark-env.sh<br>export JAVA_HOME=/usr/java/jdk1.8.0_151<br>export SPARK_MASTER_IP=node1<br>export SPARK_MASTER_PORT=7077<br>export SPARK_WORKER_CORES=1  :一个cpu线程<br>export SPARK_WORKER_INSTANCES=1  ：一个物理节点里只有一个worker进程<br>export SPARK_WORKER_MEMORY=1g    ：物理节点内存</li>
</ol>
<h4 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h4><p><code>./sbin/start-all.sh</code></p>
<h4 id="测试"><a href="#测试" class="headerlink" title="测试"></a>测试</h4><h5 id="访问"><a href="#访问" class="headerlink" title="访问"></a>访问</h5><p>node1:8080</p>
<h5 id="样例"><a href="#样例" class="headerlink" title="样例"></a>样例</h5><p>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://node1:7077 –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.6.3-hadoop2.4.0.jar 100</p>
<h5 id="2种运行模式"><a href="#2种运行模式" class="headerlink" title="2种运行模式"></a>2种运行模式</h5><ul>
<li>client：在shell中可见，就在Driver上执行</li>
<li>cluster:在浏览器客户端访问，在worker上执行<br>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master spark://node1:7077 –deploy-mode cluster  –executor-memory 1G –total-executor-cores 1 ./lib/spark-examples-1.6.3-hadoop2.4.0.jar 100</li>
</ul>
<h4 id="HA的配置"><a href="#HA的配置" class="headerlink" title="HA的配置"></a>HA的配置</h4><ul>
<li><p>spark_env.sh</p>
<p>export SPARK_DAEMON_JAVA_OPTS=”-Dspark.deploy.recoveryMode=ZOOKEEPER -Dspark.deploy.zookeeper.url=node1:2181,node2:2181,node3:2181”<br>复制到各个节点机<br>并在node2上，将master改为node2</p>
</li>
<li><p>启动zookeeper</p>
</li>
<li>启动spark</li>
<li>在node2上启动start-maseter.sh</li>
<li>查看ndoe1:8080与node2:8080</li>
<li>测试<br>kill掉node1上的master，从8080上看，是否可以切换。</li>
<li>shell测试<br>./bin/spark-shell -master spark://node1:7077</li>
</ul>
<h3 id="Yarn集群模式"><a href="#Yarn集群模式" class="headerlink" title="Yarn集群模式"></a>Yarn集群模式</h3><ul>
<li>yarn访问8088</li>
</ul>
<h4 id="配置spark-env-sh"><a href="#配置spark-env-sh" class="headerlink" title="配置spark-env.sh"></a>配置spark-env.sh</h4><figure class="highlight sh"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">xport HADOOP_CONF_DIR=/home/hadoop-2.5/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> YARN_CONF_DIR=/home/hadoop-2.5/etc/hadoop</span><br><span class="line"><span class="built_in">export</span> SPARK_HOME=/home/spark-1.6</span><br><span class="line"><span class="built_in">export</span> SPARK_JAR=/home/spark-1.6/lib/spark-assembly-1.6.3-hadoop2.4.0.jar</span><br><span class="line"><span class="built_in">export</span> PATH=<span class="variable">$SPARK_HOME</span>/bin:<span class="variable">$PATH</span></span><br></pre></td></tr></table></figure>
<h4 id="测试-1"><a href="#测试-1" class="headerlink" title="测试"></a>测试</h4><ul>
<li>注意，此处无需启动，yarn的source Manager就代替了master,node manager代替worker</li>
<li><p>启动zookeeper(./zkServer.start),hadoop(start-all.sh)</p>
</li>
<li><p>client模式<br> ./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-client  ./lib/spark-examples-1.6.3-hadoop2.4.0.jar 100</p>
</li>
<li><p>cluster模式<br>./bin/spark-submit –class org.apache.spark.examples.SparkPi –master yarn-cluster  ./lib/spark-examples-1.6.3-hadoop2.4.0.jar 100</p>
</li>
</ul>
<h4 id="spark在yarn下的调度"><a href="#spark在yarn下的调度" class="headerlink" title="spark在yarn下的调度"></a>spark在yarn下的调度</h4><p><img src="26.spark的yarn调度.png" alt="spark的yarn调度"><br>由driver分配任务，然后从resourceManager申请资源，ResourceManager返回NodeManager，driver再与NodeManager通信，NodeManager分配Containor给Driver，Driver分配Task到Containor中运行。<br>这里Driver充当的是ApplicationManager的角色，MR ON YARN也是如此。</p>
<h2 id="【重点】调度源码分析"><a href="#【重点】调度源码分析" class="headerlink" title="【重点】调度源码分析"></a>【重点】调度源码分析</h2><h3 id="概览"><a href="#概览" class="headerlink" title="概览"></a>概览</h3><p><img src="31.spark源码解析概览.png" alt="spark源码解析概览"><br>spark-submit –&gt; Driver进程 –&gt; SparkConext –&gt; 创建2个对象DAGScheduler/Taskduler<br>Tasksheduler会通过它对应的一个后台进程，去连接Master，向Master注册Application<br>Master –&gt; worker –&gt; executor –&gt; executor会反向注册到TaskScheduler<br>这就完成了SparkContex的初始化，继续代码运行。<br>DAGSheduler –&gt; Job切割成Stage(使用堆由底切割，由头运行) –&gt; stage切割成task放在taskSet中交由TaskScheduler<br>TaskScheduler会找到出每个Task的preferLocation，然后提交到executor进行运行</p>
<h3 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h3><p><img src="32.DAG的Stage切割核心代码.png" alt="DAG的Stage切割核心代码"><br><img src="33.源码-1.png" alt="源码-1"></p>
<p><img src="33.源码-2.png" alt="源码-2"></p>
<p><img src="33.源码-3.png" alt="源码-3"></p>
<p><img src="33.源码-4.png" alt="源码-4"></p>
<h3 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h3><p>如何知道的知道的location</p>
<h2 id="【重点】宽窄依赖及DAG（有向无环图）"><a href="#【重点】宽窄依赖及DAG（有向无环图）" class="headerlink" title="【重点】宽窄依赖及DAG（有向无环图）"></a>【重点】宽窄依赖及DAG（有向无环图）</h2><p><img src="27.依据宽依赖的切割.png" alt="依据宽依赖的切割"></p>
<ul>
<li>这里stage3中有3个task，因为在stage3的reduce端从map端的partition去取数据，然后进行运算。</li>
<li>为什么有A–&gt;B阶段：因为要减少多机到多机的shuffle，否则出错重算代价高</li>
</ul>
<h3 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h3><p>有shuffle就是宽依赖，多机到多机的传递<br>窄依赖没有通过网络传递数据:数据本地性原则</p>
<h3 id="用处"><a href="#用处" class="headerlink" title="用处"></a>用处</h3><p>用于切割Job<br>Driver可以认为是一个application（应用程序），见到action则划分一个job<br>job也需要跑在分布式上，job会根据宽依赖来切割为stage<br>是基于数据本地性的切割<br>一个stage内的窄依赖进行pipleline操作：每条线都是一个pipleline,每个pipleline都是一个task，即每个stage都有多个task组成，task是最小的运算单元<br>尽量避免shuffle：多机到多机依赖高，容易出错，每次错误，都需要重新算<br>为什么会有B？<br>将7到3化分为3×3，4×3</p>
<h2 id="并行度"><a href="#并行度" class="headerlink" title="并行度"></a>并行度</h2><ul>
<li>数据并行【任务并行度】度其实就是task的数量，就是一个rdd中有多少个partition：RDD中的partition为计算最小单位</li>
<li><p>资源并行度可以认为是支持的executor×线程数。</p>
</li>
<li><p>一般rdd数据是资源并行度的2-3倍<br>local[2]  sc.parallelize(names, 4);</p>
</li>
<li><p>node<br>spark_worker_instances:节点上跑几个worker</p>
</li>
<li><p>worker<br>spark_worker_memory<br>worker上跑多少进程</p>
</li>
<li><p>executor<br>spark_executor_memory</p>
</li>
<li><p>写代码时，需要对集群中的资源并发度情况有数，才能对任务并发度进行控制。</p>
</li>
</ul>
<h2 id="【重点】算子"><a href="#【重点】算子" class="headerlink" title="【重点】算子"></a>【重点】算子</h2><h3 id="转换算子与操作算子"><a href="#转换算子与操作算子" class="headerlink" title="转换算子与操作算子"></a>转换算子与操作算子</h3><p><img src="28.转换算子与操作算子.png" alt="转换算子与操作算子"></p>
<ul>
<li>对RDD的操作都是在从节点中执行</li>
<li>由于不是在一台机器，RDD匿名类中访问外部的变量，都需要变成final</li>
</ul>
<h3 id="详解"><a href="#详解" class="headerlink" title="详解"></a>详解</h3><ul>
<li><p>map<br>map是对集合内所有数据进行逐个操作</p>
</li>
<li><p>mapPartitions<br>也是map，就是一次处理一个partition的数据<br>注意driver中的数据需要是final才能传递给executor去执行</p>
</li>
<li><p>mapPartitionsWithIndex<br>这个可以拿到每个partition的index</p>
</li>
<li><p>map2Pair<br>对键值对进行map</p>
</li>
<li><p>flatmap<br>flatmap = map + flat<br>对rdd中的数据逐个进行处理，处理完的结果是序列，然后把这些每个结果的序列合并在一起。<br>flatmap 与map都是逐个对元素进行处理，不同之处在于：map处理完之后返回仍然是单个元素，即：map是一到一的操作，flatmap处理完之后返回是一个集合，即flatmap是一到多的操作</p>
</li>
<li><p>filter<br>过滤</p>
</li>
<li><p>coalesce算子合并<br>fiter之后使partitions中的数据不平均，可能造成数据倾斜<br>用coalesce减少一下partition，减少数据倾斜的症状<br>coalesce中的shuffle参数<br>可以是true，这种情况下，可以用于partition个数的增多。</p>
</li>
<li><p>repartition<br>经典场景，是增加并发度</p>
</li>
<li><p>collect<br>collect是将全部数据返回到driver端去执行，容易内存溢出<br>foreach是在worker节点打出</p>
</li>
<li><p>count</p>
</li>
<li><p>groupByKey<br>groupByKey是将key相同的数据，value合并成一个序列。<br>rdd数量与上游rdd数量相同<br>但如果设置了conf.set(“spark.default.parallelism”, 5)shuffle时就执行会读取默认的，5个.<br>key值为3个，10个partition，如何分？取模，key并没有被分到同一个reduce上<br>groupByKey算子可以传参,决定shuffle的reduce阶段的partition的并发（reduce个数）</p>
</li>
<li><p>reduce（action）<br> 算子里的逻辑在从节点，但结果在driver端。reduce本身是在driver端，但里边的逻辑是executor中执行。</p>
</li>
<li><p>reduceByKey=groupByKey + reduce<br>spark里的reduceByKey在map端自带combiner(map端的partition中，局部进行累加)<br>效率高过 单步的groupByKey + reduce</p>
</li>
<li><p>aggregateByKey<br>平均不能直接用reduceByKey来做，因为其自带combiner，局部的平均会使总体的平均错误。<br>aggregateByKey可以分别指定map端与reduce端的操作<br>第一参数是，每个key值的初始值<br>第二参数是 Seq Function,如何进行shuffle map-side的本地聚合<br>第三参数是 如何进行shuffle reduce-side的全局聚合</p>
</li>
<li><p>Sample<br>随机采样smple(withReplcement:Boolen, fraction Double),第一参数是否可重复采样。</p>
</li>
<li><p>Take(action)<br>取前n个，然后变成一个list</p>
</li>
<li><p>takesame = smple + take</p>
</li>
<li><p>union<br>2个rdd的partition合并在一起</p>
</li>
<li><p>dinstinct（shuffle）<br>去重</p>
</li>
<li><p>sortByKey<br>按key升序排序，false参数是倒叙</p>
</li>
<li><p>SaveAsTextFile<br>存到本地或者hdfs上，hdfs，9000端口是完全分布式，8020是高可用</p>
</li>
<li><p>intersection<br>2个rdd元素取交集，还可以去重</p>
</li>
<li><p>cartesian<br> 2个rdd元素的笛卡尔积，排列组合</p>
</li>
<li><p>CountByKey=Count + GroupByKey（action）<br>每个key对应的cout得出</p>
</li>
<li><p>cogroup<br>将两个rdd的可以相同的合并到一起，这两个rddvalue值不同，形成2个序列作为值。</p>
</li>
<li><p>join<br>将key相同两个rdd，value进行笛卡尔积</p>
</li>
</ul>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><ul>
<li>textFile读取时并发情况<br>textFile函数第二个参数有最小partition的设置,min(设置，资源并行度)<br>从local读基本依据最小partition<br>从hdfs默认有多少个block就有多少个partition</li>
<li>从代码看stage的划分<br><img src="35.2个shffule分成3个stage.png" alt="2个shffule分成3个stage"></li>
</ul>
<h2 id="实例："><a href="#实例：" class="headerlink" title="实例："></a>实例：</h2><h3 id="WorldCount"><a href="#WorldCount" class="headerlink" title="WorldCount"></a>WorldCount</h3><ul>
<li>匿名内部类</li>
</ul>
<h3 id="pi代码"><a href="#pi代码" class="headerlink" title="pi代码"></a>pi代码</h3><p><img src="34.pi.png" alt="pi"></p>
<h3 id="topN"><a href="#topN" class="headerlink" title="topN"></a>topN</h3><p>sort + take</p>
<h3 id="groupTopN"><a href="#groupTopN" class="headerlink" title="groupTopN"></a>groupTopN</h3><p>groupByKey<br>mapToPair,在此处进行排序，并取N<br>forEach输出即可</p>
<h3 id="二次排序"><a href="#二次排序" class="headerlink" title="二次排序"></a>二次排序</h3><p>第一列相同，比较第二列<br>都需要自定义key</p>
<h2 id="效率"><a href="#效率" class="headerlink" title="效率"></a>效率</h2><h3 id="共享变量"><a href="#共享变量" class="headerlink" title="共享变量"></a>共享变量</h3><ul>
<li>简介<br>算子会每个task都会封装共同的变量，这样效率低<br>共享变量会使数据每个executor中共享</li>
<li>广播变量<br>hadoop里的conf会广播出去<br><img src="35.broadcast.png" alt="broadcast"></li>
<li>累加器accumulators<br>并行累加，是global的，在driver端进行计数<br>sparkContext.accumulator(v)<br>在从节点中无法读到<br><img src="35.accumulator.png" alt="accumulator"></li>
<li>这里可以通过sleep来看ui（4040）</li>
</ul>
<h1 id="SparkSQL"><a href="#SparkSQL" class="headerlink" title="SparkSQL"></a>SparkSQL</h1><h2 id="复习"><a href="#复习" class="headerlink" title="复习"></a>复习</h2><p>sparksql是建立在sparkCore基础上的，在rdd与各个数据存储之间通过dataframe来建立关联，使处理起来更容易。</p>
<h2 id="Architecture-of-Spark-SQL"><a href="#Architecture-of-Spark-SQL" class="headerlink" title="Architecture of Spark SQL"></a>Architecture of Spark SQL</h2><p>DataFrame可以理解为一张表<br><img src="36.SparkSQL.png" alt="SparkSQL"></p>
<h2 id="SparkSQL底层"><a href="#SparkSQL底层" class="headerlink" title="SparkSQL底层"></a>SparkSQL底层</h2><p>sqlParser(解析器),Analyzer（分析器）,Optimizer（优化器）<br>SparkSQL旧版本就是Shark<br> <img src="38.SparkSQL底层.png" alt="SparkSQL底层"><br><img src="39.SparkSQL的优化.png" alt="SparkSQL的优化"></p>
<h2 id="scala的隐式转换"><a href="#scala的隐式转换" class="headerlink" title="scala的隐式转换"></a>scala的隐式转换</h2><h3 id="隐式方法"><a href="#隐式方法" class="headerlink" title="隐式方法"></a>隐式方法</h3><p>scala可以通过声明为implicit的函数，将一个类对象转变为另一个类对象<br><img src="40.scala的隐式转换implicit.png" alt="scala的隐式转换implicit"></p>
<h3 id="隐式参数"><a href="#隐式参数" class="headerlink" title="隐式参数"></a>隐式参数</h3><p>参数的传入也可以隐式的规定<br>implicit val signPen = new SignPen();</p>
<p>def signForExame(name:val)(implicit signPen:SignPen)<br>类似于C的全局变量</p>
<h2 id="【重点】DataFrame"><a href="#【重点】DataFrame" class="headerlink" title="【重点】DataFrame"></a>【重点】DataFrame</h2><h3 id="简介-1"><a href="#简介-1" class="headerlink" title="简介"></a>简介</h3><p>与RDD类似，也是一个分布式数据容器，但更像传统数据库的表格，除了数据以外，还掌握数据的结构信息，及schema<br>存储时，列式存储：方便分析，以列为单位方便统计，不需要加载其他列。<br><img src="37.rdd与dataframe.png" alt="rdd与dataframe"></p>
<h3 id="操作"><a href="#操作" class="headerlink" title="操作"></a>操作</h3><p>parquet:是本地的列式存储文件</p>
<ul>
<li>load:读取内建文件</li>
<li>save:保存内建文件<br>第一个参数：文件名<br>第二个参数：<br> SaveMode.ErrorIfExists<br> SaveMode.Ignore（文件存在就不存了）<br> SaveMode.Append（同一路径可以追加）<br> SaveMode.Overwrite</li>
<li>format(“json”):指定保存，读取的格式</li>
</ul>
<h3 id="RDD与DataFram反射互转"><a href="#RDD与DataFram反射互转" class="headerlink" title="RDD与DataFram反射互转"></a>RDD与DataFram反射互转</h3><ul>
<li>createDataFrame(RDD, .class)</li>
<li>registerTempTable(“table.name”)【临时表】</li>
<li>DataFrame.toJavaRDD()，转成RDD</li>
<li>Row.getAs(“列名”)</li>
</ul>
<h3 id="RDD与DataFrame动态转换"><a href="#RDD与DataFrame动态转换" class="headerlink" title="RDD与DataFrame动态转换"></a>RDD与DataFrame动态转换</h3><ul>
<li>创建JavaRDD<code>&lt;Row&gt;</code></li>
<li>创建schema:StructType–&gt;DataTypes.createStructType(fields)–&gt;List<code>&lt;StructField&gt;</code></li>
<li>createDataFrame</li>
<li>registerTemTable</li>
</ul>
<p>应用：<br>  有些时候不确定哪些列，需要从配置文件等地方读出来</p>
<h2 id="【重点】数据源"><a href="#【重点】数据源" class="headerlink" title="【重点】数据源"></a>【重点】数据源</h2><h3 id="Json"><a href="#Json" class="headerlink" title="Json"></a>Json</h3><p>Json读取的2种方法<br>Json2DataFrame<br>DataFrame2RDD<br>RDD算子Join<br>RDD2DataFrame<br>DataFrame2Json<br>完成了从Json到Json的变化</p>
<h3 id="jdbc"><a href="#jdbc" class="headerlink" title="jdbc"></a>jdbc</h3><h4 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h4><p>spark的思想都是lazy的，读取的时候并不会读，操作时才会<br>jdbc 2 DataFrame<br>DataFrame 2 RDD<br>RDD join map filter<br>RDD 2 DataFrame<br>DataFrame 2 RDD map 2 insert to jdbc// 数量大时Mysql扛不住，后边说修改</p>
<h4 id="测试-2"><a href="#测试-2" class="headerlink" title="测试"></a>测试</h4><ul>
<li>需要将mysql的jar包放到spark上<br>lib放到spark的lib下</li>
<li>cluster模式需要配置spark-env.sh</li>
<li>yarn模式需要在spark-defaults.con配置【大一统】<br>配置classpath，driver以及executor</li>
<li><p>con<br>yarn 8088<br><img src="41.jdbc运行.png" alt="jdbc运行"></p>
</li>
<li><p>standalone client<br>./bin/spark-submit –master spark://node1:7077 –class com.sun.spark.JDBCDataSource ./sparksql.jar</p>
</li>
<li><p>standalone cluster<br>hadoop fs -put sparksql.jar /<br>./bin/spark-submit –master spark://node1:7077 –deploy-mode cluster –class com.sun.spark.JDBCDataSource ./sparksql.jar</p>
</li>
<li><p>yarn client<br>./bin/spark-submit –master yarn-client –class com.sun.spark.JDBCDataSource ./sparksql.jar</p>
</li>
<li><p>yarn cluster<br>./bin/spark-submit –master yarn-cluster cluster –class com.sun.spark.JDBCDataSource ./sparksql.jar</p>
</li>
</ul>
<h3 id="Hive"><a href="#Hive" class="headerlink" title="Hive"></a>Hive</h3><h4 id="搭建-1"><a href="#搭建-1" class="headerlink" title="搭建"></a>搭建</h4><ol>
<li>把hive里边的hive-site.xml放到spark/conf目录下</li>
<li>启动hive</li>
<li>启动Mysql</li>
<li>启动HDFS</li>
<li>初始化HiveContext</li>
<li>打包运行</li>
</ol>
<h4 id="步骤-1"><a href="#步骤-1" class="headerlink" title="步骤"></a>步骤</h4><p>hive的操作更像sql，不需要注册临时表，可以直接使用<br>Create Table<br>Load<br>select c1,c2,c3 from t1 join t2 on t1.c1=t2.c1 where t1.c2&gt;80<br>DataFrame.saveAsTable<br><img src="35.Hive数据源.png" alt=""><br>./bin/spark-submit –master spark://node1:7077 –class com.shsxt.study.sql.HiveDataSource ./sparksqlhive20161202.jar</p>
<h4 id="其他-1"><a href="#其他-1" class="headerlink" title="其他"></a>其他</h4><p>spark-submit参数<br> –supervise(重试)<br> –executor-memory MEM :memory for executor<br> –executor-cores NUM: Number of cores(thread) pre executor</p>
<p>Yarn<br> –num-executors NUM:直接告诉有多少线程</p>
<p> – yarn cluster时需要3个包，–jars<br> – yarn中默认执行2次</p>
<h4 id="开窗函数（分组TopN）"><a href="#开窗函数（分组TopN）" class="headerlink" title="开窗函数（分组TopN）"></a>开窗函数（分组TopN）</h4><ul>
<li>就是使用SparkSQL来做到分组取TopN</li>
<li>row_number() OVER(PARTITION BY category ORDER BY revenue DESC) rank</li>
<li>只对Hive有用，不好测，需要大家Hive来测试</li>
<li>sparksql模式并行度200<br>相当于设置conf.set(“spark.default.parallism”,”200”)<br>一开始读的时候取决于block，但经过shuffle后，会变成200<br><img src="35.开窗函数.png" alt="开窗函数"></li>
</ul>
<h2 id="UDF"><a href="#UDF" class="headerlink" title="UDF"></a>UDF</h2><p>=&gt;代表匿名函数,括号里边是输入，Int是输出</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sqlContext.udf.register(&quot;strlength&quot;, (str:string) =&gt; str.length+10))</span><br><span class="line"></span><br><span class="line">val myUDF =(str:String) =&gt;&#123;</span><br><span class="line">  str.length + 10&#125;</span><br><span class="line"></span><br><span class="line">sqlContex.udf.register(&quot;strlength&quot;, myUDF)</span><br><span class="line"></span><br><span class="line">namesDF.registerTemTable(&quot;names&quot;)</span><br><span class="line">sqlContex.sql(&quot;select name,strLength(name) from names&quot;).collect().foreache(println)</span><br></pre></td></tr></table></figure>
<h2 id="UDAF-多个进-返回一个"><a href="#UDAF-多个进-返回一个" class="headerlink" title="UDAF(多个进,返回一个)"></a>UDAF(多个进,返回一个)</h2><p>函数 def  【def 函数名（val 形参名：形参类型）:输出类型={ 函数体}】<br>类 class<br>对象 obj<br>变量 var val<br>sqltext.udf.register(“strGroupCount”,new StringGroupCount)</p>
<p>// 输入类型<br>class StringGroupCount extends UserDefinedAggregateFunction{<br>    def inputSchema:StructType={</p>
<pre><code>}
</code></pre><p>}<br>// 中间结果类型<br>// 最后的类型<br>// 局部累加（相当于combiner）update<br>// 全局累加(相当于reduce)merge<br>// 返回类型evaluate()<br><img src="35.UDAF-1.png" alt="UDAF-1"><br><img src="35.UDAF-2.png" alt="UDAF-2"><br><img src="35.UDAF-3.png" alt="UDAF-3"></p>
<h2 id="【重点】实例："><a href="#【重点】实例：" class="headerlink" title="【重点】实例："></a>【重点】实例：</h2><h3 id="pageRank"><a href="#pageRank" class="headerlink" title="pageRank"></a>pageRank</h3><p>split(“\s+”)，多个空格切分<br>mapValue方法，键不变对值进行操作<br>case(urls,rank)是一个匹配，如果是键值对形式走这个函数体，反之不走<br>urls.map是<br>可以算出最重要的节点</p>
<ul>
<li>action<br>action前有太多的for循环太多，会是DAG太大</li>
<li>将数据checkpoint到磁盘，</li>
<li>效率低<br><img src="42.pageRank.png" alt="pageRank1"><br><img src="42.pageRank-2.png" alt="pageRank2"></li>
</ul>
<h1 id="SparkStream"><a href="#SparkStream" class="headerlink" title="SparkStream"></a>SparkStream</h1><h2 id="复习-1"><a href="#复习-1" class="headerlink" title="复习"></a>复习</h2><p>DStream的算子操作类似与rdd算子操作，不同之处它们的数据来源不太相同，DStream来自流</p>
<h2 id="简介-2"><a href="#简介-2" class="headerlink" title="简介"></a>简介</h2><ul>
<li><p>流处理与批处理<br>流处理像楼梯，批处理像电梯（攒数据）</p>
</li>
<li><p>sparkStream是微流处理<br>准实时，延迟要求不高<br>im.qq.com<br>天猫双十一 JStorm<br>春运实时迁徙<br>延迟为秒级，storm是毫秒级</p>
</li>
<li><p>其他流处理框架<br>storm /Trident(storm来的) / Spark Stream / Flink 流</p>
</li>
<li><p>spark Stream 架构<br><img src="43.spark Stream架构.png" alt=""><br>数据被封装为DStream，以时间切割为RDD<br>DStream也有很多算子，相当于于将算子应用到每个RDD上，再应用到每个元素上<br>DSteam是流，没有大小，只有时间</p>
<p>// yum install nc<br>// nc -lk 8888</p>
</li>
<li><p>JavaReceiverInputStream 会占有一个execotor,local[2]至少为2才可以</p>
</li>
</ul>
<h2 id="wordcount"><a href="#wordcount" class="headerlink" title="wordcount"></a>wordcount</h2><p><img src="44.StreamWorldCount.png" alt="StreamWorldCount1"><br><img src="44.StreamWorldCount-2.png" alt="StreamWorldCount2"></p>
<h2 id="HDFS数据源"><a href="#HDFS数据源" class="headerlink" title="HDFS数据源"></a>HDFS数据源</h2><ul>
<li>从HDFS读数据<br>JavaDStream<code>&lt;String&gt;</code> lines = jssc.textFileStream(“hdfs://node2:8020/wordCount_dir”)<br>从hdfs文件夹下读取数据，只要有文件进来就计算<br>这里没有receiver 所以local可以为1</li>
<li>数据结果保存<br>synchronized 锁</li>
<li>优化<br>foreachRDD<br>连接池</li>
</ul>
<h2 id="算子"><a href="#算子" class="headerlink" title="算子"></a>算子</h2><ul>
<li>updateStateByKey<br>每次记住上次的结果，在上次结果之上进行累加<br>RDD1:(K1,V1)<br>RDD2:(K1,V2) (K1, V1+V2)<br>必须要做checkPoint<br><img src="45.updateStateByKey.png" alt="updateStateByKey"></li>
<li>transform<br>可以对RDD进行操作，在里边可以进行任何的算子操作，返回一个RDD即可<br><img src="45.Tranforme.png" alt="Tranforme1"><br><img src="45.Tranforme-2.png" alt="Tranforme2"></li>
</ul>
<h2 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h2><p>  <img src="46.存储.png" alt="存储"><br>  <img src="46.数据库连接池.png" alt="数据库连接池"></p>
<h2 id="kafka"><a href="#kafka" class="headerlink" title="kafka"></a>kafka</h2><ul>
<li><p>简介<br>kafka是一个分布式消息队列，先将数据打到卡夫卡中，SparkStream从卡夫卡中取数据<br>其实用于缓存流数据</p>
</li>
<li><p>修改conf/server.properties<br>kroker.id=0[第一台为0，2台为1]<br>log.dirs=/kafka-logs[往卡夫卡里放的数据，不能是临时的]<br>*num.partitions=1[topic 有多个partition组成 类似与RDD]<br>log.retention.hours=168[根据时间删除，过168小时删除]<br>zookeeper.connect=node1:2181,node2:2181,node3:2181</p>
</li>
<li><p>bin下sh加执行权限</p>
</li>
<li><p>scp到其他节点即可</p>
</li>
<li><p>启动<br>nohup ./bin/kafka-server-start.sh config/server.properties &gt; kafka.log 2&gt;&amp;1<br>标准错误也重定向到标准输出中</p>
</li>
<li><p>测试<br>创建<br>./bin/kafka-topics.sh –create –zookeeper node1:2181,node2:2181,node3:2181 –topic 20180108 –partitions 2 –replication-factor 2<br>这个topic有2个partition<br>副本数2<br>元数据放在zookeeper中，数据放在卡夫卡集群</p>
<p>显示<br>./bin/kafka-topics.sh –list  –zookeeper node1:2181,node2:2181,node3:2181<br> ./bin/kafka-topics.sh –describe  –zookeeper node1:2181,node2:2181,node3:2181</p>
<p>测试打数据<br>kafka-console-producer.sh<br>kafka-console-consumer.sh<br>./bin/kafka-console-producer.sh –broker-list node1:9092,node2:9092,node3:9092 –topic 20180108 进入交互，输入数据即可，指定节点<br>./bin/kafka-console-consumer.sh –zookeeper node1:2181,node2:2181,node3:2181 –topic 20180108 可以看到数据</p>
</li>
<li><p>卡夫卡里的数据是键值对，val是我们存的值<br><img src="47.kafkaWordCount.png" alt="kafkaWordCount"></p>
</li>
</ul>
<h2 id="两种读数据的区别"><a href="#两种读数据的区别" class="headerlink" title="两种读数据的区别"></a>两种读数据的区别</h2><h3 id="receiver"><a href="#receiver" class="headerlink" title="receiver"></a>receiver</h3><p>WAL 预写日志，将内存里的数据写到磁盘中<br>这种方式在读取不同的topic时，需要不同的receiver<br>Kafaka partion 不等于 RDD partition</p>
<h3 id="Direct"><a href="#Direct" class="headerlink" title="Direct"></a>Direct</h3><p>需要人工指定消息队列里的offset<br>1.Kafka patition = RDD partition<br>2.不需要WAL，可以从kafka中重新读取<br>3.只计算一次，offset存在checkpoint中，第一种存在zookeeper<br>4.需要自己去控制offset<br>  <img src="47.directiStream读取方式.png" alt="directiStream读取方式"></p>

      
    </div>

    <div>
      
        

      
    </div>

    <div>
      
        

      
    </div>


    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/Hadoop/" rel="tag"># Hadoop</a>
          
        </div>
      

      
        
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2018/01/04/Hadoop生态-Hive、HBase/" rel="next" title="Hadoop生态:Hive、HBase">
                <i class="fa fa-chevron-left"></i> Hadoop生态:Hive、HBase
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2018/07/15/2018-7-15http协议基础/" rel="prev" title="http协议基础">
                http协议基础 <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </article>



    <div class="post-spread">
      
    </div>
  </div>

          
          </div>
          


          
  <div class="comments" id="comments">
    
  </div>


        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap" >
            文章目录
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview">
            站点概览
          </li>
        </ul>
      

      <section class="site-overview sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
          <img class="site-author-image" itemprop="image"
               src="/images/curiosity.jpg"
               alt="sun" />
          <p class="site-author-name" itemprop="name">sun</p>
           
              <p class="site-description motion-element" itemprop="description">有一片天空，能留下鸟的痕迹</p>
          
        </div>
        <nav class="site-state motion-element">
        
          
            <div class="site-state-item site-state-posts">
              <a href="/archives">
                <span class="site-state-item-count">35</span>
                <span class="site-state-item-name">日志</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-categories">
              <a href="/categories">
                <span class="site-state-item-count">2</span>
                <span class="site-state-item-name">分类</span>
              </a>
            </div>
          

          
            <div class="site-state-item site-state-tags">
              <a href="/tags">
                <span class="site-state-item-count">19</span>
                <span class="site-state-item-name">标签</span>
              </a>
            </div>
          

        </nav>

        
          <div class="feed-link motion-element">
            <a href="/atom.xml" rel="alternate">
              <i class="fa fa-rss"></i>
              RSS
            </a>
          </div>
        

        <div class="links-of-author motion-element">
          
        </div>

        
        

        
        

        


      </section>

      
      <!--noindex-->
        <section class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
              
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#简介"><span class="nav-number">1.</span> <span class="nav-text">简介</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#支持3种API"><span class="nav-number">1.1.</span> <span class="nav-text">支持3种API</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4种模式"><span class="nav-number">1.2.</span> <span class="nav-text">4种模式</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#回顾MR"><span class="nav-number">2.</span> <span class="nav-text">回顾MR</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#sparkCore"><span class="nav-number">3.</span> <span class="nav-text">sparkCore</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#RDD"><span class="nav-number">3.1.</span> <span class="nav-text">RDD</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#（Resilient-Distributed-Dataset）弹性分布式数据集"><span class="nav-number">3.1.1.</span> <span class="nav-text">（Resilient Distributed Dataset）弹性分布式数据集</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#特性"><span class="nav-number">3.1.2.</span> <span class="nav-text">特性</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD容错"><span class="nav-number">3.1.3.</span> <span class="nav-text">RDD容错</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#术语解释"><span class="nav-number">3.2.</span> <span class="nav-text">术语解释</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#调度概览"><span class="nav-number">3.3.</span> <span class="nav-text">调度概览</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#运行时"><span class="nav-number">3.3.1.</span> <span class="nav-text">运行时</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#流程示意图"><span class="nav-number">3.3.2.</span> <span class="nav-text">流程示意图</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#持久化策略"><span class="nav-number">3.4.</span> <span class="nav-text">持久化策略</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#序列化"><span class="nav-number">3.4.1.</span> <span class="nav-text">序列化</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#搭建"><span class="nav-number">3.5.</span> <span class="nav-text">搭建</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#单机模式"><span class="nav-number">3.5.1.</span> <span class="nav-text">单机模式</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#standalone集群模式"><span class="nav-number">3.5.2.</span> <span class="nav-text">standalone集群模式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置文件"><span class="nav-number">3.5.2.1.</span> <span class="nav-text">配置文件</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#启动"><span class="nav-number">3.5.2.2.</span> <span class="nav-text">启动</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试"><span class="nav-number">3.5.2.3.</span> <span class="nav-text">测试</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#访问"><span class="nav-number">3.5.2.3.1.</span> <span class="nav-text">访问</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#样例"><span class="nav-number">3.5.2.3.2.</span> <span class="nav-text">样例</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#2种运行模式"><span class="nav-number">3.5.2.3.3.</span> <span class="nav-text">2种运行模式</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#HA的配置"><span class="nav-number">3.5.2.4.</span> <span class="nav-text">HA的配置</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Yarn集群模式"><span class="nav-number">3.5.3.</span> <span class="nav-text">Yarn集群模式</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#配置spark-env-sh"><span class="nav-number">3.5.3.1.</span> <span class="nav-text">配置spark-env.sh</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试-1"><span class="nav-number">3.5.3.2.</span> <span class="nav-text">测试</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#spark在yarn下的调度"><span class="nav-number">3.5.3.3.</span> <span class="nav-text">spark在yarn下的调度</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】调度源码分析"><span class="nav-number">3.6.</span> <span class="nav-text">【重点】调度源码分析</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#概览"><span class="nav-number">3.6.1.</span> <span class="nav-text">概览</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#代码"><span class="nav-number">3.6.2.</span> <span class="nav-text">代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#问题"><span class="nav-number">3.6.3.</span> <span class="nav-text">问题</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】宽窄依赖及DAG（有向无环图）"><span class="nav-number">3.7.</span> <span class="nav-text">【重点】宽窄依赖及DAG（有向无环图）</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#区别"><span class="nav-number">3.7.1.</span> <span class="nav-text">区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用处"><span class="nav-number">3.7.2.</span> <span class="nav-text">用处</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#并行度"><span class="nav-number">3.8.</span> <span class="nav-text">并行度</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】算子"><span class="nav-number">3.9.</span> <span class="nav-text">【重点】算子</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#转换算子与操作算子"><span class="nav-number">3.9.1.</span> <span class="nav-text">转换算子与操作算子</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#详解"><span class="nav-number">3.9.2.</span> <span class="nav-text">详解</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#其他"><span class="nav-number">3.10.</span> <span class="nav-text">其他</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#实例："><span class="nav-number">3.11.</span> <span class="nav-text">实例：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#WorldCount"><span class="nav-number">3.11.1.</span> <span class="nav-text">WorldCount</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#pi代码"><span class="nav-number">3.11.2.</span> <span class="nav-text">pi代码</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#topN"><span class="nav-number">3.11.3.</span> <span class="nav-text">topN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#groupTopN"><span class="nav-number">3.11.4.</span> <span class="nav-text">groupTopN</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#二次排序"><span class="nav-number">3.11.5.</span> <span class="nav-text">二次排序</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#效率"><span class="nav-number">3.12.</span> <span class="nav-text">效率</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#共享变量"><span class="nav-number">3.12.1.</span> <span class="nav-text">共享变量</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkSQL"><span class="nav-number">4.</span> <span class="nav-text">SparkSQL</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#复习"><span class="nav-number">4.1.</span> <span class="nav-text">复习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Architecture-of-Spark-SQL"><span class="nav-number">4.2.</span> <span class="nav-text">Architecture of Spark SQL</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#SparkSQL底层"><span class="nav-number">4.3.</span> <span class="nav-text">SparkSQL底层</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#scala的隐式转换"><span class="nav-number">4.4.</span> <span class="nav-text">scala的隐式转换</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#隐式方法"><span class="nav-number">4.4.1.</span> <span class="nav-text">隐式方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#隐式参数"><span class="nav-number">4.4.2.</span> <span class="nav-text">隐式参数</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】DataFrame"><span class="nav-number">4.5.</span> <span class="nav-text">【重点】DataFrame</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#简介-1"><span class="nav-number">4.5.1.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#操作"><span class="nav-number">4.5.2.</span> <span class="nav-text">操作</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD与DataFram反射互转"><span class="nav-number">4.5.3.</span> <span class="nav-text">RDD与DataFram反射互转</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RDD与DataFrame动态转换"><span class="nav-number">4.5.4.</span> <span class="nav-text">RDD与DataFrame动态转换</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】数据源"><span class="nav-number">4.6.</span> <span class="nav-text">【重点】数据源</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#Json"><span class="nav-number">4.6.1.</span> <span class="nav-text">Json</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#jdbc"><span class="nav-number">4.6.2.</span> <span class="nav-text">jdbc</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#步骤"><span class="nav-number">4.6.2.1.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#测试-2"><span class="nav-number">4.6.2.2.</span> <span class="nav-text">测试</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Hive"><span class="nav-number">4.6.3.</span> <span class="nav-text">Hive</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#搭建-1"><span class="nav-number">4.6.3.1.</span> <span class="nav-text">搭建</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#步骤-1"><span class="nav-number">4.6.3.2.</span> <span class="nav-text">步骤</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#其他-1"><span class="nav-number">4.6.3.3.</span> <span class="nav-text">其他</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#开窗函数（分组TopN）"><span class="nav-number">4.6.3.4.</span> <span class="nav-text">开窗函数（分组TopN）</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UDF"><span class="nav-number">4.7.</span> <span class="nav-text">UDF</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#UDAF-多个进-返回一个"><span class="nav-number">4.8.</span> <span class="nav-text">UDAF(多个进,返回一个)</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#【重点】实例："><span class="nav-number">4.9.</span> <span class="nav-text">【重点】实例：</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#pageRank"><span class="nav-number">4.9.1.</span> <span class="nav-text">pageRank</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#SparkStream"><span class="nav-number">5.</span> <span class="nav-text">SparkStream</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#复习-1"><span class="nav-number">5.1.</span> <span class="nav-text">复习</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#简介-2"><span class="nav-number">5.2.</span> <span class="nav-text">简介</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#wordcount"><span class="nav-number">5.3.</span> <span class="nav-text">wordcount</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#HDFS数据源"><span class="nav-number">5.4.</span> <span class="nav-text">HDFS数据源</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#算子"><span class="nav-number">5.5.</span> <span class="nav-text">算子</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#存储"><span class="nav-number">5.6.</span> <span class="nav-text">存储</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#kafka"><span class="nav-number">5.7.</span> <span class="nav-text">kafka</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#两种读数据的区别"><span class="nav-number">5.8.</span> <span class="nav-text">两种读数据的区别</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#receiver"><span class="nav-number">5.8.1.</span> <span class="nav-text">receiver</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Direct"><span class="nav-number">5.8.2.</span> <span class="nav-text">Direct</span></a></li></ol></li></ol></li></ol></div>
            

          </div>
        </section>
      <!--/noindex-->
      

      

    </div>
  </aside>


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright" >
  
  &copy; 
  <span itemprop="copyrightYear">2019</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">sun</span>
</div>


<div class="powered-by">
  由 <a class="theme-link" href="https://hexo.io">Hexo</a> 强力驱动
</div>

<div class="theme-info">
  主题 -
  <a class="theme-link" href="https://github.com/iissnan/hexo-theme-next">
    NexT.Pisces
  </a>
</div>


        

        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    
    
  </div>

  

<script type="text/javascript">
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>









  




  
  <script type="text/javascript" src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script type="text/javascript" src="/lib/fastclick/lib/fastclick.min.js?v=1.0.6"></script>

  
  <script type="text/javascript" src="/lib/jquery_lazyload/jquery.lazyload.js?v=1.9.7"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>

  
  <script type="text/javascript" src="/lib/fancybox/source/jquery.fancybox.pack.js?v=2.1.5"></script>


  


  <script type="text/javascript" src="/js/src/utils.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/motion.js?v=5.1.0"></script>



  
  


  <script type="text/javascript" src="/js/src/affix.js?v=5.1.0"></script>

  <script type="text/javascript" src="/js/src/schemes/pisces.js?v=5.1.0"></script>



  
  <script type="text/javascript" src="/js/src/scrollspy.js?v=5.1.0"></script>
<script type="text/javascript" src="/js/src/post-details.js?v=5.1.0"></script>



  


  <script type="text/javascript" src="/js/src/bootstrap.js?v=5.1.0"></script>



  



  




	





  





  





  



  
  

  

  

  

  


  

</body>
</html>
